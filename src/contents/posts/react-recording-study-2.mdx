---
title: "[시리즈 2편] React에서 녹음 기능 구현하기 - Web Audio API와 MediaRecorder API"
description: ""
startDate: "2025-09-27"
tags: ["React", "오디오 녹음", "Web Audio API", "MediaRecorder API", "AudioContext", "시리즈"]
category: "posts"
featured: true
published: true
---

## AudioContext: 오디오 세계의 관리자

지난 1편에서 Navigator를 통해 브라우저가 마이크에 접근하는 방법을 배웠다면, 이번에는 실제로 오디오를 다루는 **AudioContext**에 대해 알아보자.

AudioContext를 가장 쉽게 이해하는 방법은 **오디오 작업을 총괄하는 관리자**로 생각하는 것이다. 마치 음향 엔지니어가 녹음실에서 여러 장비들을 조작하듯이, AudioContext는 브라우저에서 모든 오디오 작업을 관리한다.

```javascript
// 오디오 관리자 생성
const audioContext = new AudioContext();
console.log("관리자 상태:", audioContext.state); // 'suspended' 또는 'running'
```

AudioContext를 처음 만들면 보통 `suspended`(일시정지) 상태로 시작한다. 이는 브라우저가 사용자를 보호하기 위해서이다.

```javascript
// 사용자가 버튼을 클릭했을 때만 오디오 시작
button.addEventListener("click", async () => {
  if (audioContext.state === "suspended") {
    await audioContext.resume(); // 이제 진짜 시작
    console.log("오디오 관리자 일 시작");
  }
});
```

## 오디오 노드: 레고 블록처럼 조립하는 오디오

Web Audio API의 가장 재미있는 부분은 **오디오 노드(Audio Node)** 시스템이다. 이를 레고 블록에 비유하면 이해하기 쉽다. 각각의 블록은 하나의 기능을 담당하고, 이들을 연결해서 원하는 결과를 만들어낸다.

### 오디오 노드의 세 가지 역할

오디오 노드는 크게 세 가지 역할로 나눌 수 있다:

1. **입력(Input)**: 소리가 들어오는 곳
   - 마이크에서 들어오는 소리
   - 파일에서 재생되는 음악
   - 컴퓨터가 만들어내는 신호음

2. **가공(Effects)**: 소리를 변화시키는 곳
   - 볼륨을 크게/작게 만들기
   - 에코나 잔향 효과 넣기
   - 소리를 분석해서 데이터 뽑아내기

3. **출력(Destination)**: 소리가 나가는 곳
   - 스피커나 헤드폰으로 내보내기

```javascript
// 1. 관리자 생성
const audioContext = new AudioContext();

// 2. 마이크 소리를 가져오기 (입력)
const source = audioContext.createMediaStreamSource(stream);

// 3. 볼륨 조절기 만들기 (가공)
const gainNode = audioContext.createGain();
gainNode.gain.value = 0.5; // 볼륨 50%

// 4. 소리 분석기 만들기 (가공)
const analyser = audioContext.createAnalyser();

// 5. 레고 블록처럼 연결하기
source.connect(gainNode); // 마이크 → 볼륨조절
gainNode.connect(analyser); // 볼륨조절 → 분석
analyser.connect(audioContext.destination); // 분석 → 스피커
```

> 처음에는 복잡해 보이지만, 이 방식의 장점은 **유연성**이다. 마치 레고로 집을 짓듯이, 필요에 따라 다양한 조합을 만들 수 있다. 녹음할 때는 스피커 연결을 빼고, 이펙트를 추가하고 싶으면 중간에 노드를 하나 더 끼우면 된다.

실제 코드의 `useAudioMeter` 훅에서 이런 연결 과정을 볼 수 있다:

## MediaRecorder: 소리를 파일로 저장하는 녹음기

Web Audio API가 소리를 실시간으로 분석하고 가공하는 도구라면, **MediaRecorder API**는 그 소리를 실제로 파일로 저장하는 녹음기이다.

### 녹음은 어떻게 동작할까?

MediaRecorder의 동작 방식은 정말 간단하다:

1. **소리 들어옴**: 마이크에서 실시간으로 소리가 들어온다
2. **작은 조각으로 나누기**: 긴 녹음을 작은 덩어리(청크)로 나눈다
3. **파일 조각 만들기**: 각 덩어리를 파일 형태로 변환한다
4. **조각들 모으기**: 녹음이 끝나면 모든 조각을 합쳐서 완전한 파일을 만든다

```javascript
// 녹음기 만들기
const mediaRecorder = new MediaRecorder(stream, {
  mimeType: "audio/webm", // 어떤 형식으로 저장할지
});

// 파일 조각들을 담을 바구니
const chunks = [];

// 조각이 만들어질 때마다 바구니에 담기
mediaRecorder.ondataavailable = (event) => {
  if (event.data && event.data.size > 0) {
    chunks.push(event.data);
    console.log("새로운 조각", event.data.size, "바이트");
  }
};

// 녹음이 끝나면 조각들을 하나로 합치기
mediaRecorder.onstop = () => {
  const completeFile = new Blob(chunks, { type: "audio/webm" });
  const downloadUrl = URL.createObjectURL(completeFile);
  console.log("완성된 파일", downloadUrl);
};

// 녹음 시작 (0.1초마다 조각 만들기)
mediaRecorder.start(100);
```

### 왜 조각으로 나눌까?

**실시간 처리의 비밀**
녹음을 작은 조각으로 나누는 이유는 **실시간 처리** 때문이다. 만약 10분짜리 녹음을 통째로 처리하려면 메모리도 많이 쓰고, 녹음 중간에 뭔가 문제가 생기면 모든 걸 잃을 수 있다. 하지만 작은 조각으로 나누면:

- 메모리를 효율적으로 사용할 수 있다
- 실시간으로 서버에 전송할 수 있다
- 문제가 생겨도 일부분만 잃는다
- 진행 상황을 사용자에게 보여줄 수 있다

## 브라우저별 호환성

| 브라우저 | 좋아하는 형식 | 특징                        |
| -------- | ------------- | --------------------------- |
| Chrome   | `audio/webm`  | 웹엠 형식을 좋아함          |
| Firefox  | `audio/ogg`   | 오그 형식을 선호            |
| Safari   | `audio/mp4`   | 엠피포 형식만 알아듣기도 함 |

## 모든 것을 하나로: 실제 사용 예시

이제 배운 것들을 모두 합쳐서 실제로 사용하는 방법을 보자. 실제 코드의 `RecordModal`에서 이런 패턴을 볼 수 있다:

```javascript
function RecordingApp() {
  // 1. 각각의 기능을 담당하는 훅들
  const { audioContext, isRunning, resume } = useAudioContext();
  const { stream, isEnabled, enable } = useMicrophone();
  const { level } = useAudioMeter({ audioContext, stream });
  const { isRecording, start, stop } = useMediaRecorder(stream);

  // 2. 녹음 버튼을 눌렀을 때
  const handleRecord = async () => {
    // 먼저 오디오 관리자를 깨우기
    if (!isRunning) await resume();
    // 그 다음 마이크 켜기
    if (!isEnabled) await enable();

    if (!isRecording) {
      start(); // 녹음 시작!
    } else {
      const audioFile = await stop(); // 녹음 끝내고 파일 받기
      // 이제 파일을 다운로드하거나 서버에 보낼 수 있다
    }
  };

  return (
    <div>
      <div>현재 소리 크기: {Math.round(level * 100)}%</div>
      <button onClick={handleRecord}>{isRecording ? "녹음 정지" : "녹음 시작"}</button>
    </div>
  );
}
```

## 다음 편 예고

Web Audio API와 MediaRecorder API의 기본 동작 원리를 쉽게 이해했다면, 이제 실제 데이터가 어떻게 처리되는지 더 자세히 알아볼 차례이다. **[시리즈 3편]**에서는 Blob이라는 파일 저장 방식, 소리를 숫자로 분석하는 FFT, 그리고 메모리를 효율적으로 사용하는 TypedArray에 대해 알아보겠다.  
처음에는 어려워 보였지만 실제로도 쉽지 않았다. 하지만 하나씩 계속 보다 보니 조금씩 이해가 가기 시작한다.
